{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPProxyAuth\n",
    "import time\n",
    "session = requests.Session()\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from .env file\n",
    "load_dotenv('/home/wjones/CC/Capstone/tbd2/Track/.env', override=True)\n",
    "\n",
    "proxy_host = os.getenv('proxy_host')\n",
    "proxy_port = os.getenv('proxy_port')\n",
    "proxy_user = os.getenv('proxy_user')\n",
    "proxy_pass = os.getenv('proxy_pass')\n",
    "\n",
    "proxies = {\n",
    "    \"http\": f\"http://{proxy_user}:{proxy_pass}@{proxy_host}:{proxy_port}/\",\n",
    "    \"https\": f\"http://{proxy_user}:{proxy_pass}@{proxy_host}:{proxy_port}/\"\n",
    "}\n",
    "\n",
    "\n",
    "url = 'https://www.tfrrs.org/results_search.html'\n",
    "\n",
    "response = session.get(url, proxies=proxies)\n",
    "html = response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# FILEPATH: /home/wjones/CC/Capstone/tbd2/experiment_notebooks/web_scraping.ipynb\n",
    "base_url = 'https://www.tfrrs.org/results_search_page.html'\n",
    "params = {\n",
    "    'page': 1,  # Start with page 1\n",
    "    'search_query': '',  # Assuming you don't need a specific search query\n",
    "    'with_month': '',  # Assuming you don't need a specific month\n",
    "    'with_sports': 'track',  # Assuming you don't need a specific sport\n",
    "    'with_states': '',  # Assuming you don't need a specific state\n",
    "    'with_year': '2023'  # Set the year you want to filter by\n",
    "}\n",
    "\n",
    "# Open the CSV file in append mode\n",
    "with open('csv/races_urls.csv', 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    \n",
    "    while True:\n",
    "        response = session.get(base_url, params=params)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        rows = soup.find_all('tr')\n",
    "        for row in rows:\n",
    "            a_tag = row.find('a')\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                meet_name = a_tag.text.strip()\n",
    "                meet_url = f\"https://www.tfrrs.org{a_tag.attrs['href']}\"\n",
    "                \n",
    "                # Write the meet name and URL to the CSV file\n",
    "                writer.writerow([meet_name, meet_url])\n",
    "\n",
    "        next_page_link = soup.find('a', rel='next')  # Adjust if the website uses different attributes\n",
    "        if next_page_link and 'href' in next_page_link.attrs:\n",
    "            params['page'] += 1  # Increment the page number\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        time.sleep(.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = soup.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meets = []\n",
    "for row in rows:\n",
    "    # Find the <a> tag in this row\n",
    "    a_tag = row.find('a')\n",
    "    if a_tag and 'href' in a_tag.attrs:\n",
    "        # Extract the URL and the meet name\n",
    "        url = a_tag.attrs['href']\n",
    "        meet_name = a_tag.text.strip()\n",
    "        \n",
    "        # Construct the full URL if needed (if the URL is relative)\n",
    "        full_url = f\"https://www.tfrrs.org{url}\" if url.startswith('/') else url\n",
    "        \n",
    "        # Append the data to your list\n",
    "        meets.append({\n",
    "            'meet_name': meet_name,\n",
    "            'tfrrs_url': full_url\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meet_name': 'LSU Purple Tiger', 'tfrrs_url': 'https://www.tfrrs.org/results/76417/LSU_Purple_Tiger'}\n",
      "{'meet_name': 'Bearson-Gathje Classic', 'tfrrs_url': 'https://www.tfrrs.org/results/76704/Bearson-Gathje_Classic'}\n",
      "{'meet_name': '2023 Rod McCravy Memorial Track & Field', 'tfrrs_url': 'https://www.tfrrs.org/results/76211/2023_Rod_McCravy_Memorial_Track__Field_'}\n",
      "{'meet_name': 'Potts Invitational', 'tfrrs_url': 'https://www.tfrrs.org/results/75281/Potts_Invitational_'}\n",
      "{'meet_name': '2023 Graduate Classic', 'tfrrs_url': 'https://www.tfrrs.org/results/75972/2023_Graduate_Classic'}\n",
      "{'meet_name': 'Dutch Early Bird Pentathlon', 'tfrrs_url': 'https://www.tfrrs.org/results/75985/Dutch_Early_Bird_Pentathlon'}\n",
      "{'meet_name': 'Jimmy Carnes Invitational (College/College-Age)', 'tfrrs_url': 'https://www.tfrrs.org/results/75574/Jimmy_Carnes_Invitational_College_College-Age'}\n",
      "{'meet_name': 'Monmouth Midwest Indoor Invitational', 'tfrrs_url': 'https://www.tfrrs.org/results/76732/_Monmouth_Midwest_Indoor_Invitational_'}\n",
      "{'meet_name': '2023 Louisville Cardinal Classic Indoor', 'tfrrs_url': 'https://www.tfrrs.org/results/76006/2023_Louisville_Cardinal_Classic_Indoor'}\n",
      "{'meet_name': '2023 BYU Cougar Indoor Invitational', 'tfrrs_url': 'https://www.tfrrs.org/results/76855/2023_BYU_Cougar_Indoor_Invitational'}\n",
      "{'meet_name': 'COLLEGE THROWS COMPETITION', 'tfrrs_url': 'https://www.tfrrs.org/results/76030/College_Throws_Competition'}\n",
      "{'meet_name': 'VCU RAMS Indoor Invitational', 'tfrrs_url': 'https://www.tfrrs.org/results/75451/VCU_RAMS_Indoor_Invitational'}\n",
      "{'meet_name': 'Friends University First Chance Qualifier', 'tfrrs_url': 'https://www.tfrrs.org/results/75926/Friends_University_First_Chance_Qualifier'}\n",
      "{'meet_name': 'Ward Haylett Invitational', 'tfrrs_url': 'https://www.tfrrs.org/results/76444/Ward_Haylett_Invitational'}\n",
      "{'meet_name': 'Ed Temple Classic', 'tfrrs_url': 'https://www.tfrrs.org/results/76709/Ed_Temple_Classic'}\n",
      "{'meet_name': 'Wolverine Invite', 'tfrrs_url': 'https://www.tfrrs.org/results/75977/Wolverine_Invite'}\n",
      "{'meet_name': 'Father Diamond 2023', 'tfrrs_url': 'https://www.tfrrs.org/results/76241/Father_Diamond_2023'}\n",
      "{'meet_name': 'USM II', 'tfrrs_url': 'https://www.tfrrs.org/results/76002/USM_II'}\n",
      "{'meet_name': '2023 Phoenix Invitational', 'tfrrs_url': 'https://www.tfrrs.org/results/76034/2023_Phoenix_Invitational'}\n",
      "{'meet_name': 'Illini Classic', 'tfrrs_url': 'https://www.tfrrs.org/results/76343/Illini_Classic'}\n",
      "{'meet_name': 'West Point Open', 'tfrrs_url': 'https://www.tfrrs.org/results/75502/West_Point_Open_'}\n",
      "{'meet_name': 'Wagner Invitational', 'tfrrs_url': 'https://www.tfrrs.org/results/75431/Wagner_Invitational'}\n"
     ]
    }
   ],
   "source": [
    "for meet in meets:\n",
    "    print(meet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "events_we_care_about = ['800 Meters', '1500 Meters', '3000 Steeplechase', '5000 Meters', '10,000 Meters']\n",
    "\n",
    "# Open the CSV written abobe\n",
    "with open('csv/races_urls.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    meet_urls = [row[1] for row in reader]  # the url should be in the second column...\n",
    "results = []\n",
    "\n",
    "for meet_url in meet_urls:\n",
    "    time.sleep(.5) # server rest\n",
    "    response = session.get(meet_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    tables = soup.find_all('table', class_='tablesaw')\n",
    "    for table in tables:\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            a_tag = row.find('a')\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                event_name = a_tag.text.strip()\n",
    "                if any(event in event_name for event in events_we_care_about): # check if the event is one we care about\n",
    "                    event_url = a_tag.attrs['href']\n",
    "                    results.append({\n",
    "                        'meet_url': meet_url,\n",
    "                        'event_name': event_name,\n",
    "                        'event_url': event_url\n",
    "                    })\n",
    "with open('csv/sections.csv', 'w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['meet_url', 'event_name', 'event_url'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_660112/1860254007.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('csv/sections.csv')\n",
    "results = []\n",
    "\n",
    "for url in df.iloc[:, 2]:\n",
    "    response = session.get(url) # get the html using the proxy session\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', class_='tablesaw')\n",
    "    if table:\n",
    "        rows = table.find_all('tr')[1:]  # first skip the header row\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 4:  # Ensure there are enough columns\n",
    "                place = cols[0].text.strip()\n",
    "                athlete_name = cols[1].text.strip()\n",
    "                athlete_url = cols[1].find('a')['href'] if cols[1].find('a') else None\n",
    "                time = cols[4].text.strip()\n",
    "\n",
    "                # string split the url on '/', the meet id is the third last element\n",
    "                # NOTE the meet id in track races is the second numeric element and is 7 digits long\n",
    "                # NOTE the meet id tffrs uses is not the same as the DB meet ID which is a primary key\n",
    "                meet_id = url.split('/')[-3] if url else None\n",
    "                result = {\n",
    "                    'meet_id': meet_id,\n",
    "                    'athlete_url': athlete_url,\n",
    "                    'place': place,\n",
    "                    'time': time\n",
    "                }\n",
    "                results.append(result)\n",
    "                #print(result)\n",
    "\n",
    "# write results to put in the raceResults table.\n",
    "with open('csv/raceResults.csv', 'w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['meet_id', 'athlete_url', 'place', 'time'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the CSV files\n",
    "df_sections = pd.read_csv('csv/sections.csv')\n",
    "df_meet = pd.read_csv('csv/races_urls.csv')\n",
    "\n",
    "# Merge the dataframes on the 'meet_url' column\n",
    "df_merged = pd.merge(df_sections, df_meet, on='meet_url')\n",
    "\n",
    "# Write the merged dataframe back to 'sections.csv'\n",
    "df_merged.to_csv('csv/races.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "months = [\n",
    "    'January', 'February', 'March', 'April', 'May', 'June',\n",
    "    'July', 'August', 'September', 'October', 'November', 'December'\n",
    "]\n",
    "\n",
    "with open('csv/races_urls.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    meet_data = [row for row in reader]\n",
    "\n",
    "updated_meet_data = []\n",
    "\n",
    "for meet_name,meet_url in meet_data[1:]:\n",
    "    # get the HTML\n",
    "    response = session.get(meet_url, proxies=proxies)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract the race_id from the URL \n",
    "        # NOTE the race_id is the general id for the meet, not the section\n",
    "        #meet_id = event_url.split('/')[-3]\n",
    "\n",
    "        # Date and location are located in 'panel-heading-normal-text'\n",
    "        elements = soup.find_all(class_='panel-heading-normal-text')\n",
    "        \n",
    "        date = None\n",
    "        location = None\n",
    "        # BUG: the location could contain a date, for example \"Augustana\" contains \"August\"\n",
    "        for element in elements:\n",
    "            text = element.get_text(strip=True)\n",
    "            # Checks if any month is in the text and if the text contains a number\n",
    "            if any(month in text for month in months) and ',' in text and re.search(r'\\d', text):  \n",
    "                date = text\n",
    "            elif '-' in text:  # locations should always be dashed\n",
    "                location = text\n",
    "        updated_meet_data.append([meet_name, meet_url, date, location])\n",
    "        #print([meet_name, meet_url, meet_id, date, location])\n",
    "    else:\n",
    "        print(f\"Could not retrieve {meet_url}. Code: {response.status_code}\")\n",
    "    \n",
    "    time.sleep(.5) # Server rest\n",
    "\n",
    "with open('csv/updated_race_urls.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['meet_name', 'meet_url', 'date', 'location'])  # Write header\n",
    "    writer.writerows(updated_meet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
